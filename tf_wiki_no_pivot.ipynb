{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from itertools import combinations\n",
    "\n",
    "import datasets\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update language-specific wiki40b datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki40b/en (download: Unknown size, generated: 9.75 GiB, post-processed: Unknown size, total: 9.75 GiB) to C:\\Users\\onurg\\.cache\\huggingface\\datasets\\wiki40b\\en\\1.1.0\\d15702fbf830e65fd775c50946364ff0c02fd3089b31887fabf97c2dad970760...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.38k/1.38k [00:00<00:00, 343kB/s]\n",
      "Downloading:  39%|███▉      | 3.67G/9.42G [09:21<13:16, 7.23MB/s]"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "\n",
    "dataset_wb = load_dataset(\"wiki40b\", language)\n",
    "dataset_wb = concatenate_datasets([dataset_wb[split] for split in ('train', 'test', 'validation')])\n",
    "dataset_wb = dataset_wb.remove_columns(\"version_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-730e2fce18f82c32\n",
      "Reusing dataset csv (C:\\Users\\onurg\\.cache\\huggingface\\datasets\\csv\\default-730e2fce18f82c32\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    }
   ],
   "source": [
    "language = \"de\"\n",
    "\n",
    "dataset_wb = load_dataset(\"csv\", data_files=f\"data/{language}_raw.csv\", split=\"train\")\n",
    "dataset_wb = dataset_wb.remove_columns(\"version_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n_START_ARTICLE_\\nNational Park (Dorf)\\n_START_SECTION_\\nNamensherkunft\\n_START_PARAGRAPH_\\nDer Name des Dorfes bezieht sich auf den Tongariro National Park, Neuseelands ersten Nationalpark, der 1894 eingerichtet wurde und an dessen westlicher Grenze sich das Dorf befindet.\\n_START_SECTION_\\nGeographie\\n_START_PARAGRAPH_\\nDas Dorf befindet sich rund 18\\xa0km nordwestlich des Gipfels des 2797\\xa0m hohen aktiven Vulkans Ruapehu und damit an seinen nordwestlichen Ausläufern.\\n_START_SECTION_\\nBevölkerung\\n_START_PARAGRAPH_\\nZum Zensus des Jahres 2013 zählte das Dorf 174\\xa0Einwohner, 27,5\\xa0% weniger als zur Volkszählung im Jahr 2006.\\n_START_SECTION_\\nTourismus\\n_START_PARAGRAPH_\\nDominanter Wirtschaftszweig des Dorfes ist der Tourismus. Zahlreiche Unternehmungen haben sich hier niedergelassen, um Ausrüstung, Wandertouren oder Beherbergung für Touristen bereitzustellen. Das Dorf ist Ausgangspunkt für zahlreiche geführte Touren sowie von Shuttlebussen zum 15 km entfernten größten Skigebiet des Landes, Whakapapa, an den Ausläufern Ruapehu.\\n_START_SECTION_\\nStraßenverkehr\\n_START_PARAGRAPH_\\nNational Park liegt am New Zealand State Highway 4, der das Dorf an seiner Ostseite in Nord-Süd-Richtung passiert. Am südöstlichen Ende des Dorfes zweigt der New Zealand State Highway 47 vom State Highway 4 in Richtung Turangi am Lake Taupo ab.\\n_START_SECTION_\\nSchienenverkehr\\n_START_PARAGRAPH_\\nDas Dorf besitzt mit einem kleinen Bahnhof einen Anschluss an den North Island Main Trunk Railway. Knapp 6 km nördlich befindet sich die Raurimu-Spirale, eine Strecke der Eisenbahnlinie, in der durch mehrfache engen zusammenliegenden Kehrschleife der Höhenunterschied zum Central Volcanic Plateau überwunden wird.',\n",
       " 'wikidata_id': 'Q1318354'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_wb[7]\n",
    "\n",
    "\"\"\"\n",
    "https://de.wikipedia.org/wiki/National_Park_(Dorf)\n",
    "\n",
    "'\\n_START_ARTICLE_\\nNational Park (Dorf)\\n_START_SECTION_\\nNamensherkunft\\n_START_PARAGRAPH_\\nDer Name des Dorfes bezieht sich auf den Tongariro National Park,....'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_ARTICLE_\", \"\")\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_PARAGRAPH_\", \"\")\n",
    "    \n",
    "    example[\"text\"] = example[\"text\"].replace(\"_NEWLINE_\", \" \")\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_SECTION_\", \" \")\n",
    "\n",
    "    _, _, example['title'], *text_lst  = example['text'].split(\"\\n\")\n",
    "    example['text'] = \" \".join(text_lst)\n",
    "    example[\"text\"] = example[\"text\"].replace(\"\\xa0\", \" \")\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_wb = dataset_wb.map(process, num_proc=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Brand Hogefeld (* in Wismar; † 1496 in Lübeck) war ein deutscher Kaufmann und Ratsherr der Hansestadt Lübeck.   Leben  Brand Hogefeld war Ältermann der Bergenfahrer in Lübeck. Er vertrat die Bergenfahrer 1478 (gemeinsam mit dem Sekretär des Hansekontors in Bergen Theodericus Brandes) bei König Christian I. von Dänemark in Kopenhagen und 1479 auf dem Tag der Wendischen Städte in Lübeck. Er wurde 1479 in den Lübecker Rat erwählt. Er wurde vom Lübecker Rat nach Bryggen in Bergen gesandt. 1484 verhandelte er erneut in Kopenhagen wegen der Privilegien der Hanse in Dänemark und in Norwegen. Beim Hansetag 1487 in Lübeck erhielt er den Auftrag zwischen den Abgesandten der Hansestädte Deventer und Kampen einen Vergleich zu finden. Er vermittelte auch zwischen den Bergenfahrern und dem Hansekontor in Brügge. Hogefeld wohnte in Lübeck in der Beckergrube 12.',\n",
       " 'wikidata_id': 'Q23061875',\n",
       " 'title': 'Brand Hogefeld'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_wb[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "\n",
    "path = os.path.join(root_dir, data_dir, language)\n",
    "updated_wb.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Updated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "#languages = ('fr', 'it', 'de')\n",
    "languages = ('fr', 'it', 'de', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, data_dir)\n",
    "\n",
    "#updated_en = load_from_disk(os.path.join(root_dir, data_dir, \"en\"))\n",
    "list_updated_others = [load_from_disk(os.path.join(root_dir, data_dir, language)) for language in languages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Operations as English Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique articles in other languages than English: 2995399\n",
      "number of articles that appear in every languages excluding English: 222976\n",
      "number of articles that appear in English and at least one other language: 1077115\n"
     ]
    }
   ],
   "source": [
    "# Set of titles in English\n",
    "en_id = set(updated_en['wikidata_id'])\n",
    "\n",
    "# Set of titles in every other language than English\n",
    "#other_id_list = [set(lng['wikidata_id']) for lng in list_updated_others]\n",
    "other_id_list = [set(lng['wikidata_id']) for lng in list_updated_others]\n",
    "other_union = set.union(*other_id_list)\n",
    "\n",
    "# Articles that appear in every languages excluding English\n",
    "other_intersection = set.intersection(*other_id_list)\n",
    "\n",
    "# Articles that appear in English and at least one other language\n",
    "all_intersection = en_id.intersection(other_union)\n",
    "\n",
    "print(f\"number of unique articles in other languages than English: {len(other_union)}\")\n",
    "print(f\"number of articles that appear in every languages excluding English: {len(other_intersection)}\")\n",
    "print(f\"number of articles that appear in English and at least one other language: {len(all_intersection)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3253/3253 [02:06<00:00, 25.76ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Update \"other\" language datasets: changing text field, removing title\n",
    "list_updated_others = [dataset.rename_column(\"text\", f\"text_{language}\") for dataset, language in zip(list_updated_others, languages)]\n",
    "list_updated_others = [dataset.remove_columns(\"title\") for dataset in list_updated_others]\n",
    "\n",
    "# \n",
    "updated_en = updated_en.rename_column(\"text\", \"text_en\")\n",
    "filtered_en = updated_en.filter(lambda example: example[\"wikidata_id\"] in all_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def mapping_closure(dset: datasets.Dataset, language: str) -> dict:\n",
    "    dic = {}\n",
    "\n",
    "    def get_mapping(example, language: str):\n",
    "        dic[example[\"wikidata_id\"]] =  example[f\"text_{language}\"]\n",
    "\n",
    "    dset.map(get_mapping, fn_kwargs={\"language\": language})\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1363865/1363865 [02:40<00:00, 8501.75ex/s]\n",
      "100%|██████████| 813736/813736 [01:51<00:00, 7323.35ex/s] \n",
      "100%|██████████| 1727572/1727572 [03:44<00:00, 7710.37ex/s]\n"
     ]
    }
   ],
   "source": [
    "list_mapping = [mapping_closure(dset, language) for dset, language  in zip(list_updated_others, languages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_language(example, list_mapping, languages):\n",
    "    for mapping, language in zip(list_mapping, languages):\n",
    "        example[f\"text_{language}\"] = mapping.get(example[\"wikidata_id\"])\n",
    "\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1077115/1077115 [20:16<00:00, 885.35ex/s] \n"
     ]
    }
   ],
   "source": [
    "merged_ds = filtered_en.map(merge_language, fn_kwargs={\"list_mapping\": list_mapping, \"languages\": languages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wikidata_id': 'Q278549',\n",
       " 'text_en': '  Applications  An important use for \\\\Gamma-convergence is in homogenization theory. It can also be used to rigorously justify the passage from discrete to continuum theories for materials, for example, in elasticity theory.',\n",
       " 'title': 'Γ-convergence',\n",
       " 'text_fr': None,\n",
       " 'text_it': None,\n",
       " 'text_de': ' In der Variationsrechnung bezeichnet Γ-Konvergenz (Gamma-Konvergenz) eine spezielle Konvergenzart für Funktionale. Sie wurde von Ennio de Giorgi eingeführt. Ursprünglich wurde sie als G-Konvergenz bezeichnet, da sie für greensche Funktionale entwickelt wurde. Der Begriff Γ-Konvergenz entstand durch die Verallgemeinerung dieses Konvergenzbegriffes.   Anwendungen  Eine wichtige Anwendung findet die Γ-Konvergenz in der Homogenisierungstheorie und der Dimensionsreduktion. Sie kann auch benutzt werden, um eine rigorose Begründung für den Übergang von diskreten zu kontinuierlichen Modellen zu liefern, beispielsweise bei der Elastizitätstheorie. Weitere Anwendungsgebiete sind im Bereich von Phasenübergängen und Program Slicing zu finden.   Verwandte Konvergenzbegriffe  Ein auf Banachräumen verwandter Konvergenzbegriff ist die Mosco-Konvergenz, die äquivalent ist zu gleichzeitiger Γ-Konvergenz bezüglich der Normtopologie und der schwachen Topologie.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_ds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "\n",
    "path = os.path.join(root_dir, data_dir, \"merged_small\")\n",
    "merged_ds.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(example, languages, min_sentences):\n",
    "    # TODO: change\n",
    "    if len(sent_tokenize(example[\"text_en\"])) <= min_sentences:\n",
    "        return False\n",
    "    for language in languages:\n",
    "        if example[f\"text_{language}\"] and len(sent_tokenize(example[f\"text_{language}\"])) <= min_sentences:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1078/1078 [58:45<00:00,  3.27s/ba] \n"
     ]
    }
   ],
   "source": [
    "min_sentences = 5\n",
    "\n",
    "filtered_ds = merged_ds.filter(filter_short, fn_kwargs={\"languages\": languages, \"min_sentences\": min_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wikidata_id', 'text_en', 'title', 'text_fr', 'text_it', 'text_de'],\n",
       "    num_rows: 639046\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wikidata_id': 'Q388957',\n",
       " 'text_en': \"  Profesional career  Has transferred to Fenerbahçe from Beyoğluspor in 1955. He was one of the fan favourites when he was playing. He played for Fenerbahçe between 1955–69, scoring 168 goals. He won the Turkish League 4 times and the Istanbul League title twice.   International career  Has played 37 times for Turkey, starting as captain 10 times.   Personal life  Has' brother, Mehmet Ali Has, was also a Turkish professional footballer.\",\n",
       " 'title': 'Şeref Has',\n",
       " 'text_fr': \"  Carrière de joueur  Avec le club de Fenerbahçe, il remporte notamment quatre championnats de Turquie, une Coupe de Turquie et une Coupe des Balkans. Il joue huit matchs en Coupe d'Europe des clubs champions, inscrivant trois buts dans cette compétition. Il dispute un total de 323 matchs en première division turque, pour 80 buts marqués. Il réalise sa meilleure performance lors de la saison 1958-1959, où il inscrit 14 buts en championnat.   Carrière internationale  Şeref Has compte 39 sélections et 1 but avec l'équipe de Turquie entre 1956 et 1967.  Il est convoqué pour la première fois par le sélectionneur national Cihat Arman pour un match amical contre la Pologne le 16 novembre 1956 (1-1). Par la suite, le 18 décembre 1958, il inscrit son seul but en sélection, contre la Tchécoslovaquie, lors d'un match amical (victoire 1-0). Il reçoit sa dernière sélection le 28 novembre 1967 contre le Pakistan (victoire 7-4).  Il joue trois matchs comptant pour les tours préliminaires du mondial 1962 et également cinq matchs comptant pour les tours préliminaires du mondial 1966.  Entre 1962 et 1967, il porte à dix reprises le brassard de capitaine de la sélection nationale turque.\",\n",
       " 'text_it': None,\n",
       " 'text_de': '  Verein  Şeref Has startete seine Vereinsfußballkarriere bei Beylerbeyi SK. Anschließend wechselte er zu Beyoğluspor. Im Sommer 1955 wechselte er zu Fenerbahçe Istanbul und spielte für diesen Verein 15 Spielzeiten lang. Durch seine langjährige Tätigkeit bei Fenerbahçe Istanbul wird er stark mit diesem Verein assoziiert. Er wird sowohl von den Fans wie auch von dem Verein als einer der bedeutendsten Spieler der Vereinsgeschichte aufgefasst. Mit Fenerbahçe konnte er fünfmal die türkische Meisterschaft und einmal den türkischen Fußballpokal gewinnen. Er zählte lange Zeit hinter der Fenerbahçe-Legende Lefter Küçükandonyadis als der Spieler mit den zweitmeisten Einsätzen für diesen Verein, bis beide in dieser Rubrik in den 90ern von Müjdat Yetkiner überholt wurden. Zur Saison 1968/69 entschied sich Fenerbahçe, einen radikalen Schnitt mit der Mannschaft zu machen und eine neue Mannschaft mit jungen Spielern zu formen. Infolge dieser Maßnahmen trennte man sich von vielen gestandenen Spielern, darunter auch Has. Daraufhin erklärte Has, erstmal zum Saisonende seine Karriere beenden zu wollen. Zu diesem Zweck wurde am 21. Juni 1969 ein Abschiedsspiel organisiert, in dem Fenerbahçe gegen eine Auswahl von Stars antrat. An diesem Abschiedsspiel nahmen mit Özcan Arkoç und Uwe Seeler auch zwei bekannte Größen des deutschen Fußballs teil. Has spielte später in dem Zeitraum 1973 bis 1976 für Mersin İdman Yurdu, wo er auch den Posten des Co-Trainers innehatte. Im Anschluss an diese Zeit beendete er seine aktive Spielerkarriere.   Nationalmannschaft  Şeref Has spielte zweimal für die türkische U-18-Nationalmannschaft und 39 Mal für die türkische Nationalmannschaft. Dabei lief er für Letztere in zehn Begegnungen als Kapitän auf. 1954 nahm er mit der türkischen U-18-Nationalmannschaft am FIFA-Juniorenturnier teil und erreichte hier hinter der argentinischen U-18 den 4. Platz. Has kam während des Turniers zu zwei Spieleinsätzen. Mit der türkischen Auswahl nahm er 1967 am ECO-Cup teil und konnte mit seiner Mannschaft diesen Pokal gewinnen.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/onurg/.cache/huggingface/datasets/updated_wiki40b/en\\cache-cea87e24ce22810a.arrow\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(root_dir, data_dir, \"filtered_small\")\n",
    "filtered_ds.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altering the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = (\"en\", \"fr\", \"it\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "\n",
    "def make_long(example, languages):\n",
    "    available_list = list()\n",
    "    new_example = dict()\n",
    "\n",
    "    for language in languages:\n",
    "        # TODO: check\n",
    "        if example[f\"text_{language}\"][0]:\n",
    "            available_list.append(language)\n",
    "\n",
    "    pairs = list(combinations(available_list, 2))\n",
    "\n",
    "    article1 = list()\n",
    "    article2 = list()\n",
    "\n",
    "    for lang1, lang2 in pairs:\n",
    "        article1.append(*example[f\"text_{lang1}\"])\n",
    "        article2.append(*example[f\"text_{lang2}\"])\n",
    "\n",
    "\n",
    "    new_example[\"wikidata_id\"] = example[\"wikidata_id\"] * len(pairs)  \n",
    "    new_example[\"pair\"] = [f\"{lang1}_{lang2}\" for lang1, lang2 in pairs]\n",
    "    new_example[\"article_1\"] = article1\n",
    "    new_example[\"article_2\"] = article2\n",
    "\n",
    "    return new_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 639046/639046 [09:57<00:00, 1069.16ba/s]\n"
     ]
    }
   ],
   "source": [
    "long_dataset = filtered_ds.map(make_long, fn_kwargs={\"languages\": languages}, remove_columns=filtered_ds.column_names, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article_1', 'article_2', 'pair', 'wikidata_id'],\n",
       "    num_rows: 1558009\n",
       "})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, data_dir, \"long_small_en\")\n",
    "long_dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_1': '  The sources  A tablet recovered in Nippur lists grain rations given to the messenger of a certain Šubši-mašrâ-Šakkan during Nazi-Marrutaš’ fourth year (1304 BC). There is a court order found in Ur, dated to the sixteenth year of Nazi-Maruttaš (1292 BC), in which Šubši-mašrâ-šakkan is given the title šakin māti, lúGAR KUR, “governor of the country.” It is an injunction forbidding harvesting reeds from a certain river or canal. The poetic work, Ludlul bēl nēmeqi, describes how the fortunes of Šubši-mašrâ-Šakkan, a rich man of high rank, turned one day. When beset by ominous signs, he incurred the wrath of the king, and seven courtiers plotted every kind of mischief against him. This resulted in him losing his property, “they have divided all my possessions among foreign riffraff,” friends, “my city frowns on me as an enemy; indeed my land is savage and hostile,” physical strength, “my flesh is flaccid, and my blood has ebbed away,” and health, as he relates that he “wallowed in my excrement like a sheep.” While slipping into and out of consciousness on his death bed, his family already conducting his funeral, Urnindinlugga, a kalû, or incantation priest, was sent by Marduk to presage his salvation. The work concludes with a prayer to Marduk. The text is written in the first person, leading some to speculate that the author was Šubši-mašrâ-Šakkan himself. Perhaps the only certainty is that the subject of the work, Šubši-mašrâ-Šakkan, was a significant historical person during the reign of Nazi-Maruttaš when the work was set. Of the fifty eight extant fragmentary copies of Ludlul bēl nēmeqi the great majority date to the neo-Assyrian and neo-Babylonian periods.',\n",
       " 'article_2': ' Šubši-mašrâ-šakkan (auch Šubši-mešrê-šakkan) war ein kassitischer Würdenträger während der Regierungszeit von Nazi-Maruttaš (ca. 1307–1282 v. Chr.). Ein Rationentext aus Nippur aus dem vierten Regierungsjahr von Nazimarutaš erwähnt Šubši-mašrâ-Šakkan, nach einem Text aus Ur aus dem 16. Regierungsjahr von Nazimurutaš war er \"Statthalter des Landes\"(lúGAR.KUR). Die babylonische Dichtung Ludlul bēl nēmeqi (\"Preisen will ich den Herrn der Weisheit\") nennt Šubši-mašrâ-Šakkan als den Verfasser. Dieser schildert, wie er unschuldig zahlreichen Heimsuchungen wie Krankheit, Lähmung (\"mein Haus war mein Gefängnis, mein Fleisch war wie eine Fessel, meine Arme waren nutzlos\", I.97-98,10; \"ein Dämon nutzt meinen Körper wie ein Gewand, I.71), Beschämung (\"ich lag in meinem eigenen Ausscheidungen\") und sozialer Ausgrenzung (\"mein Sklave beleidigte mich vor der Versammlung, meine Sklavin verleumdete mich vor dem Pöbel\", \"meine Freunde mieden mich\") ausgesetzt war, aber schließlich, an der Schwelle des Grabes (\"mein Grab stand offen, meine Grabbeigaben lagen bereit\", II.114) durch die Gnade Marduks errettet wurde. Es endet mit einem Hymnus auf den Stadtgott von Babylon. Der Text nennt ebenfalls den Herrscher Nazi-Maruttaš (IV, 105). Die meisten überlieferten Versionen stammen aber, dem Schriftduktus nach, aus neubabylonischer Zeit.',\n",
       " 'pair': 'en_de',\n",
       " 'wikidata_id': 'Q392634'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_dataset[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    def __init__(self, max_sentence_len, max_doc_len):\n",
    "      self.max_sentence_len = max_sentence_len\n",
    "      self.max_doc_len = max_doc_len\n",
    "\n",
    "args = Config(128,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"tokenizers/tokenizer_2_1000.json\"\n",
    "base_tokenizer = Tokenizer.from_file(file_name)\n",
    "tokenizer = BertTokenizerFast(tokenizer_object=base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = long_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, args):\n",
    "    # https://github.com/castorini/hedwig/blob/master/datasets/bert_processors/abstract_processor.py\n",
    "    # https://github.com/abhishekkrthakur/bert-entity-extraction/blob/master/src/dataset.py\n",
    "\n",
    "    def tokenize_helper(article, tokenizer, args):\n",
    "        # TODO: comment\n",
    "        sentences = [tokenizer.encode(sentence, add_special_tokens=False) for sentence in sent_tokenize(article)] \n",
    "        sentences = [sentence[:args.max_sentence_len - 2] for sentence in sentences]\n",
    "        sentences = [[tokenizer.convert_tokens_to_ids(\"[CLS]\")] + sentence + [tokenizer.convert_tokens_to_ids(\"[SEP]\")] for sentence in sentences]\n",
    "\n",
    "        sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "        mask = [[1]*sen_len for sen_len in sentence_lengths]\n",
    "\n",
    "        return sentences, mask\n",
    " \n",
    "    for i in range(1, 3):\n",
    "        \n",
    "        # example[f\"article_{i}\"] = [tokenizer.encode(sentence, \n",
    "        #                                             truncation=True,\n",
    "        #                                             add_special_tokens=True,                                            \n",
    "        #                                             max_length=args.max_sentence_len) for sentence in sent_tokenize(example[f\"article_{i}\"])]\n",
    "                                                \n",
    "        #print(example)\n",
    "\n",
    "        example[f\"article_{i}\"], example[f\"mask_{i}\"] = tokenize_helper(example[f\"article_{i}\"], tokenizer, args)\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.98ex/s]\n"
     ]
    }
   ],
   "source": [
    "#new_dataset = small_dataset.map(tokenize, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"args\": args})\n",
    "new_dataset = small_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"args\": args})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33ab4daa0c7d977fc22aa25c36b9c12b9e43d0049f266348c79f509c14309a26"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
