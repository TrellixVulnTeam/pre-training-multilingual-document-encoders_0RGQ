{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from itertools import combinations, chain\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update language-specific wiki40b datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki40b/en (download: Unknown size, generated: 9.75 GiB, post-processed: Unknown size, total: 9.75 GiB) to C:\\Users\\onurg\\.cache\\huggingface\\datasets\\wiki40b\\en\\1.1.0\\d15702fbf830e65fd775c50946364ff0c02fd3089b31887fabf97c2dad970760...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.38k/1.38k [00:00<00:00, 343kB/s]\n",
      "Downloading:  39%|███▉      | 3.67G/9.42G [09:21<13:16, 7.23MB/s]"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "\n",
    "dataset_wb = load_dataset(\"wiki40b\", language)\n",
    "dataset_wb = concatenate_datasets([dataset_wb[split] for split in ('train', 'test', 'validation')])\n",
    "dataset_wb = dataset_wb.remove_columns(\"version_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-730e2fce18f82c32\n",
      "Reusing dataset csv (C:\\Users\\onurg\\.cache\\huggingface\\datasets\\csv\\default-730e2fce18f82c32\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    }
   ],
   "source": [
    "language = \"de\"\n",
    "\n",
    "dataset_wb = load_dataset(\"csv\", data_files=f\"data/{language}_raw.csv\", split=\"train\")\n",
    "dataset_wb = dataset_wb.remove_columns(\"version_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n_START_ARTICLE_\\nNational Park (Dorf)\\n_START_SECTION_\\nNamensherkunft\\n_START_PARAGRAPH_\\nDer Name des Dorfes bezieht sich auf den Tongariro National Park, Neuseelands ersten Nationalpark, der 1894 eingerichtet wurde und an dessen westlicher Grenze sich das Dorf befindet.\\n_START_SECTION_\\nGeographie\\n_START_PARAGRAPH_\\nDas Dorf befindet sich rund 18\\xa0km nordwestlich des Gipfels des 2797\\xa0m hohen aktiven Vulkans Ruapehu und damit an seinen nordwestlichen Ausläufern.\\n_START_SECTION_\\nBevölkerung\\n_START_PARAGRAPH_\\nZum Zensus des Jahres 2013 zählte das Dorf 174\\xa0Einwohner, 27,5\\xa0% weniger als zur Volkszählung im Jahr 2006.\\n_START_SECTION_\\nTourismus\\n_START_PARAGRAPH_\\nDominanter Wirtschaftszweig des Dorfes ist der Tourismus. Zahlreiche Unternehmungen haben sich hier niedergelassen, um Ausrüstung, Wandertouren oder Beherbergung für Touristen bereitzustellen. Das Dorf ist Ausgangspunkt für zahlreiche geführte Touren sowie von Shuttlebussen zum 15 km entfernten größten Skigebiet des Landes, Whakapapa, an den Ausläufern Ruapehu.\\n_START_SECTION_\\nStraßenverkehr\\n_START_PARAGRAPH_\\nNational Park liegt am New Zealand State Highway 4, der das Dorf an seiner Ostseite in Nord-Süd-Richtung passiert. Am südöstlichen Ende des Dorfes zweigt der New Zealand State Highway 47 vom State Highway 4 in Richtung Turangi am Lake Taupo ab.\\n_START_SECTION_\\nSchienenverkehr\\n_START_PARAGRAPH_\\nDas Dorf besitzt mit einem kleinen Bahnhof einen Anschluss an den North Island Main Trunk Railway. Knapp 6 km nördlich befindet sich die Raurimu-Spirale, eine Strecke der Eisenbahnlinie, in der durch mehrfache engen zusammenliegenden Kehrschleife der Höhenunterschied zum Central Volcanic Plateau überwunden wird.',\n",
       " 'wikidata_id': 'Q1318354'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_wb[7]\n",
    "\n",
    "\"\"\"\n",
    "https://de.wikipedia.org/wiki/National_Park_(Dorf)\n",
    "\n",
    "'\\n_START_ARTICLE_\\nNational Park (Dorf)\\n_START_SECTION_\\nNamensherkunft\\n_START_PARAGRAPH_\\nDer Name des Dorfes bezieht sich auf den Tongariro National Park,....'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_ARTICLE_\", \"\")\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_PARAGRAPH_\", \"\")\n",
    "    \n",
    "    example[\"text\"] = example[\"text\"].replace(\"_NEWLINE_\", \" \")\n",
    "    example[\"text\"] = example[\"text\"].replace(\"_START_SECTION_\", \" \")\n",
    "\n",
    "    _, _, example['title'], *text_lst  = example['text'].split(\"\\n\")\n",
    "    example['text'] = \" \".join(text_lst)\n",
    "    example[\"text\"] = example[\"text\"].replace(\"\\xa0\", \" \")\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_wb = dataset_wb.map(process, num_proc=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Brand Hogefeld (* in Wismar; † 1496 in Lübeck) war ein deutscher Kaufmann und Ratsherr der Hansestadt Lübeck.   Leben  Brand Hogefeld war Ältermann der Bergenfahrer in Lübeck. Er vertrat die Bergenfahrer 1478 (gemeinsam mit dem Sekretär des Hansekontors in Bergen Theodericus Brandes) bei König Christian I. von Dänemark in Kopenhagen und 1479 auf dem Tag der Wendischen Städte in Lübeck. Er wurde 1479 in den Lübecker Rat erwählt. Er wurde vom Lübecker Rat nach Bryggen in Bergen gesandt. 1484 verhandelte er erneut in Kopenhagen wegen der Privilegien der Hanse in Dänemark und in Norwegen. Beim Hansetag 1487 in Lübeck erhielt er den Auftrag zwischen den Abgesandten der Hansestädte Deventer und Kampen einen Vergleich zu finden. Er vermittelte auch zwischen den Bergenfahrern und dem Hansekontor in Brügge. Hogefeld wohnte in Lübeck in der Beckergrube 12.',\n",
       " 'wikidata_id': 'Q23061875',\n",
       " 'title': 'Brand Hogefeld'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_wb[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "\n",
    "path = os.path.join(root_dir, data_dir, language)\n",
    "updated_wb.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Updated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "languages = ('fr', 'it', 'de', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, data_dir)\n",
    "list_updated = [load_from_disk(os.path.join(root_dir, data_dir, language)) for language in languages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr dataset has 1363865 instances.\n",
      "it dataset has 813736 instances.\n",
      "de dataset has 1727572 instances.\n",
      "en dataset has 3252407 instances.\n"
     ]
    }
   ],
   "source": [
    "for language, dataset in zip(languages, list_updated):\n",
    "    print(f\"{language} dataset has {len(dataset)} instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique articles: 5170691\n",
      "number of articles that appear at least in two languages: 1274091\n"
     ]
    }
   ],
   "source": [
    "# list of IDs of article in wikipedia corpora\n",
    "id_list = [lng['wikidata_id'] for lng in list_updated]\n",
    "ids = list(chain.from_iterable(id_list))\n",
    "\n",
    "# Unique IDs\n",
    "id_set = set(ids)\n",
    "\n",
    "# IDs that exist at least in two corpora, so that we can use them to create article pairs\n",
    "common_ids = {id for id, cnt in  Counter(ids).items()  if cnt >= 2}\n",
    "\n",
    "\n",
    "print(f\"number of unique articles: {len(id_set)}\")\n",
    "print(f\"number of articles that appear at least in two languages: {len(common_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update \"other\" language datasets: changing text field, removing title\n",
    "list_updated = [dataset.rename_column(\"text\", f\"text_{language}\") for dataset, language in zip(list_updated, languages)]\n",
    "list_updated = [dataset.remove_columns(\"title\") for dataset in list_updated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1363865/1363865 [02:37<00:00, 8673.38ex/s]\n",
      "100%|██████████| 813736/813736 [01:54<00:00, 7127.44ex/s]\n",
      "100%|██████████| 1727572/1727572 [04:07<00:00, 6992.89ex/s]\n",
      "100%|██████████| 3252407/3252407 [08:28<00:00, 6395.58ex/s]\n"
     ]
    }
   ],
   "source": [
    "temp_directory = \"../data/\"\n",
    "\n",
    "# Mapping of ID's to articles in different Wikipedia Corpora\n",
    "def mapping_closure(dset: datasets.Dataset, language: str) -> dict:\n",
    "    dic = {}\n",
    "\n",
    "    def get_mapping(example, language: str):\n",
    "        dic[example[\"wikidata_id\"]] =  example[f\"text_{language}\"]\n",
    "\n",
    "    dset.map(get_mapping, fn_kwargs={\"language\": language})\n",
    "    return dic\n",
    "\n",
    "\n",
    "#list_mapping = [mapping_closure(dset, language) for dset, language  in zip(list_updated, languages)]\n",
    "for dset, language  in zip(list_updated, languages):\n",
    "    temp_dic = mapping_closure(dset, language)\n",
    "    with open(os.path.join(temp_directory, f\"temp_dic_{language}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(temp_dic, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new datasets.Dataset which contains the \"common IDs\"\n",
    "filtered_dataset = Dataset.from_dict({\"wikidata_id\": list(common_ids)})\n",
    "path = os.path.join(root_dir, data_dir, \"only_ids\")\n",
    "filtered_dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = \"../data/\"\n",
    "root_dir = r\"C:\\Users\\onurg\\.cache\\huggingface\\datasets\"\n",
    "data_dir = \"updated_wiki40b\"\n",
    "languages = ('fr', 'it', 'de', 'en')\n",
    "\n",
    "\n",
    "path = os.path.join(root_dir, data_dir, \"only_ids\")\n",
    "filtered_dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor!! MemoryError\n",
    "\n",
    "def merge_language(example, mapping: dict, language: str):\n",
    "    example[f\"text_{language}\"] = mapping.get(example[\"wikidata_id\"])\n",
    "    return example\n",
    "\n",
    "for language in languages:\n",
    "    with open(os.path.join(temp_directory, f\"temp_dic_{language}.pkl\"), 'rb') as f:\n",
    "        print(f\"Start loading dictionary for {language}\")\n",
    "        loaded_dict = pickle.load(f)\n",
    "        print(f\"Done loading dictionary for {language}\")\n",
    "        filtered_dataset = filtered_dataset.map(merge_language, fn_kwargs={\"mapping\": loaded_dict, \"language\": language}, num_proc=4)  \n",
    "        print(f\"Done mapping instances from {language}\")\n",
    "#merged_dataset = filtered_dataset.map(merge_language, fn_kwargs={\"list_mapping\": list_mapping, \"languages\": languages}, num_proc=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, data_dir, \"filtered_small_updated\")\n",
    "filtered_dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(example, languages, min_sentences: int):\n",
    "    for language in languages:\n",
    "        if example[f\"text_{language}\"] and len(sent_tokenize(example[f\"text_{language}\"])) <= min_sentences:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Filter the articles \n",
    "min_sentences = 5\n",
    "\n",
    "filtered_dataset = filtered_dataset.filter(filter_short, fn_kwargs={\"languages\": languages, \"min_sentences\": min_sentences})\n",
    "print(f\"Number of articles in the dataset: {len(filtered_dataset)}\")\n",
    "\n",
    "path = os.path.join(root_dir, data_dir, \"filtered_small_dataset\")\n",
    "filtered_dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altering the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Creating a long dataset from the already existing wide dataset. For each Wikipedia ID, all available pair combinations are\n",
    "\n",
    "def make_long(example, languages: tuple, min_sentences: int):\n",
    "    available_list = list()\n",
    "    new_example = dict()\n",
    "\n",
    "    for language in languages:\n",
    "        # TODO: check\n",
    "        #if example[f\"text_{language}\"][0]:\n",
    "        if example[f\"text_{language}\"][0] and len(sent_tokenize(example[f\"text_{language}\"])) >= min_sentences:\n",
    "            available_list.append(language)\n",
    "\n",
    "    pairs = list(combinations(available_list, 2))\n",
    "\n",
    "    article1 = list()\n",
    "    article2 = list()\n",
    "\n",
    "    for lang1, lang2 in pairs:\n",
    "        article1.append(*example[f\"text_{lang1}\"])\n",
    "        article2.append(*example[f\"text_{lang2}\"])\n",
    "\n",
    "\n",
    "    new_example[\"wikidata_id\"] = example[\"wikidata_id\"] * len(pairs)  \n",
    "    new_example[\"pair\"] = [f\"{lang1}_{lang2}\" for lang1, lang2 in pairs]\n",
    "    new_example[\"article_1\"] = article1\n",
    "    new_example[\"article_2\"] = article2\n",
    "\n",
    "    return new_example\n",
    "\n",
    "# Filter the articles \n",
    "min_sentences = 5\n",
    "\n",
    "long_dataset = filtered_dataset.map(make_long, fn_kwargs={\"languages\": languages, \"min_sentences\": min_sentences}, \n",
    "                                    remove_columns=filtered_dataset.column_names, \n",
    "                                    batched=True, \n",
    "                                    batch_size=1)\n",
    "print(f\"Number of articles in the dataset: {len(long_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, data_dir, \"long_small_dataset\")\n",
    "long_dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_1': '  The sources  A tablet recovered in Nippur lists grain rations given to the messenger of a certain Šubši-mašrâ-Šakkan during Nazi-Marrutaš’ fourth year (1304 BC). There is a court order found in Ur, dated to the sixteenth year of Nazi-Maruttaš (1292 BC), in which Šubši-mašrâ-šakkan is given the title šakin māti, lúGAR KUR, “governor of the country.” It is an injunction forbidding harvesting reeds from a certain river or canal. The poetic work, Ludlul bēl nēmeqi, describes how the fortunes of Šubši-mašrâ-Šakkan, a rich man of high rank, turned one day. When beset by ominous signs, he incurred the wrath of the king, and seven courtiers plotted every kind of mischief against him. This resulted in him losing his property, “they have divided all my possessions among foreign riffraff,” friends, “my city frowns on me as an enemy; indeed my land is savage and hostile,” physical strength, “my flesh is flaccid, and my blood has ebbed away,” and health, as he relates that he “wallowed in my excrement like a sheep.” While slipping into and out of consciousness on his death bed, his family already conducting his funeral, Urnindinlugga, a kalû, or incantation priest, was sent by Marduk to presage his salvation. The work concludes with a prayer to Marduk. The text is written in the first person, leading some to speculate that the author was Šubši-mašrâ-Šakkan himself. Perhaps the only certainty is that the subject of the work, Šubši-mašrâ-Šakkan, was a significant historical person during the reign of Nazi-Maruttaš when the work was set. Of the fifty eight extant fragmentary copies of Ludlul bēl nēmeqi the great majority date to the neo-Assyrian and neo-Babylonian periods.',\n",
       " 'article_2': ' Šubši-mašrâ-šakkan (auch Šubši-mešrê-šakkan) war ein kassitischer Würdenträger während der Regierungszeit von Nazi-Maruttaš (ca. 1307–1282 v. Chr.). Ein Rationentext aus Nippur aus dem vierten Regierungsjahr von Nazimarutaš erwähnt Šubši-mašrâ-Šakkan, nach einem Text aus Ur aus dem 16. Regierungsjahr von Nazimurutaš war er \"Statthalter des Landes\"(lúGAR.KUR). Die babylonische Dichtung Ludlul bēl nēmeqi (\"Preisen will ich den Herrn der Weisheit\") nennt Šubši-mašrâ-Šakkan als den Verfasser. Dieser schildert, wie er unschuldig zahlreichen Heimsuchungen wie Krankheit, Lähmung (\"mein Haus war mein Gefängnis, mein Fleisch war wie eine Fessel, meine Arme waren nutzlos\", I.97-98,10; \"ein Dämon nutzt meinen Körper wie ein Gewand, I.71), Beschämung (\"ich lag in meinem eigenen Ausscheidungen\") und sozialer Ausgrenzung (\"mein Sklave beleidigte mich vor der Versammlung, meine Sklavin verleumdete mich vor dem Pöbel\", \"meine Freunde mieden mich\") ausgesetzt war, aber schließlich, an der Schwelle des Grabes (\"mein Grab stand offen, meine Grabbeigaben lagen bereit\", II.114) durch die Gnade Marduks errettet wurde. Es endet mit einem Hymnus auf den Stadtgott von Babylon. Der Text nennt ebenfalls den Herrscher Nazi-Maruttaš (IV, 105). Die meisten überlieferten Versionen stammen aber, dem Schriftduktus nach, aus neubabylonischer Zeit.',\n",
       " 'pair': 'en_de',\n",
       " 'wikidata_id': 'Q392634'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_dataset[11]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33ab4daa0c7d977fc22aa25c36b9c12b9e43d0049f266348c79f509c14309a26"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
